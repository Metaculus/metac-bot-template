ðŸ“Š Forecast Run 20250909-202815
Timestamp: 2025-09-09 20:29:29 +0300
============================================================

------------------------------------------------------------

â“ **Question:** Will humans go extinct before 2100?
ðŸ”— **URL:** https://www.metaculus.com/questions/578/

ðŸ“ **Summary**

- Final Prediction: **7.07% YES**

------------------------------------------------------------

## RESEARCH
### Reference class & base rates
- **Global Catastrophic Risks**: Historical base rate for human extinction events is effectively zero, but expert estimates suggest a 19% chance by 2100 (selection bias noted).
- **Nuclear War**: No full-scale nuclear wars have occurred; hypothetical models predict catastrophic outcomes.
- **Climate Change**: Severe climate events are increasing, but extinction-level impacts are speculative.

### Recent developments (timeline bullets)
- [2025-09-05] â†‘ â€” AI experts warn of a 20% chance of AI-induced extinction within 30 years.
- [2025-09-05] â†‘ â€” Study shows a 70% chance of Gulf Stream collapse by 2035, potentially triggering severe climate shifts.
- [2025-09-04] â†‘ â€” Article highlights nuclear war risks, with potential for massive human casualties.

### Mechanisms & drivers (causal levers)
- **Artificial Intelligence**: Large â€” Potential for AI to surpass human control.
- **Nuclear Conflict**: Large â€” Geopolitical tensions could escalate into nuclear war.
- **Climate Change**: Moderate â€” Extreme weather could destabilize ecosystems and societies.
- **Pandemics**: Moderate â€” New diseases could emerge with high mortality rates.
- **Technological Advances**: Small â€” Could mitigate or exacerbate risks.

### Differences vs. the base rate (whatâ€™s unusual now)
- **AI Development**: Rapid advancements increase existential risk, unlike historical technological changes.
- **Climate Instability**: Accelerating climate change presents new, unpredictable risks.
- **Geopolitical Tensions**: Current multipolar world order increases nuclear conflict risk.
- **Global Connectivity**: Increases pandemic spread potential compared to historical outbreaks.

### Bayesian update sketch (for the statistician)
- **Prior**: Assume a low prior probability (e.g., 1%) with moderate strength (equivalent n = 10).
- **Evidence mapping**:
  - â†‘ AI risk (large)
  - â†‘ Climate change impacts (moderate)
  - â†‘ Nuclear tensions (large)
  - â†“ Historical extinction events (small)
- **Net effect**: Posterior probability should increase moderately given recent evidence.

### Indicators to watch (leading signals; next weeks/months)
- **UP indicators**:
  - Escalation in AI capabilities without regulation.
  - Increased geopolitical tensions.
  - Accelerating climate anomalies.
- **DOWN indicators**:
  - Successful international disarmament treaties.
  - Breakthroughs in climate mitigation.
  - Effective global AI governance frameworks.

### Caveats & pitfalls
- **Uncertainty in AI timelines**: Predictions vary widely.
- **Data gaps**: Limited historical data on extinction-level events.
- **Deception risks**: Potential misinformation in geopolitical contexts.
- **Regime changes**: Political shifts could alter risk landscapes.
- **Definitional gotchas**: Ambiguities in defining "human extinction."

Final Research Summary: Recent developments in AI, climate change, and geopolitical tensions suggest a moderate increase in the probability of human extinction by 2100. However, significant uncertainties and potential mitigating factors remain.

### Market Consensus Snapshot
_Captured 2025-09-09 20:28:15 +0300 (Similarity threshold â‰¥0.60 on title match)._
**Metaculus (community forecast):**
- Will humans go extinct before 2100? â€” binary â€” 1.0% YES â€” [link](https://www.metaculus.com/questions/578/)

**Manifold (play-money odds):**
- Will humans go extinct before 2100? [Resolves to Metaculus in 2027] â€” binary â€” 4.5% YES â€” [link](https://manifold.markets/jack/will-humans-go-extinct-before-2100)

_Sources: **Metaculus** aggregates user forecasts into a community prediction; **Manifold** odds come from user betting with play-money. Treat these as noisy evidence, not ground truth; they update continuously._

------------------------------------------------------------

## GTMC1 DETAILS
_GTMC1 not used for this question._

------------------------------------------------------------

## BAYES-MC DETAILS
```json
{
  "method": "Beta weak-prior pool",
  "alpha": 0.30000000000000004,
  "beta": 3.8999999999999995,
  "p10": 8.154596744600466e-05,
  "p50": 0.019581496046109814,
  "p90": 0.2223763023823956,
  "n_members": 4
}
```

------------------------------------------------------------

## MODEL REASONING (RAW)
### OpenRouter-Default [openai/gpt-4o]
```
1. **Base Rate (Prior) Selection**
   - The historical base rate for human extinction events is effectively zero, as there have been no known instances of human extinction. However, expert estimates, such as the 19% chance by 2100 from the Global Catastrophic Risk Conference, suggest a higher probability. Given the selection bias in that estimate, a more conservative prior of 1% is reasonable, reflecting the extremely low historical frequency but acknowledging non-zero expert concerns.

2. **Comparison to Base Case**
   - Similarities: The current situation shares the historical context of no prior human extinction events.
   - Differences: Recent developments in AI, climate change, and geopolitical tensions present new risks not previously encountered, which could increase the probability of extinction.

3. **Evidence Evaluation (Likelihoods)**
   - **AI Risk**: The warning of a 20% chance of AI-induced extinction within 30 years is significant. If extinction occurs, such warnings are more likely (P(E | H) > P(E | ~H)). This evidence increases the probability.
   - **Climate Change**: The 70% chance of Gulf Stream collapse by 2035 suggests severe climate shifts. While not directly leading to extinction, it increases the risk (P(E | H) > P(E | ~H)). This evidence moderately increases the probability.
   - **Nuclear Tensions**: Highlighted risks of nuclear war with massive casualties suggest increased extinction risk (P(E | H) > P(E | ~H)). This evidence increases the probability.
   - **Historical Extinction Events**: The lack of historical extinction events suggests a low probability of extinction (P(E | H) < P(E | ~H)). This evidence decreases the probability.

4. **Bayesian Updating (Posterior Probability)**
   - Prior odds: 1% = 0.01 odds
   - Likelihood ratio for AI risk: Assume a moderate increase, e.g., 2:1
   - Likelihood ratio for climate change: Assume a small increase, e.g., 1.5:1
   - Likelihood ratio for nuclear tensions: Assume a moderate increase, e.g., 2:1
   - Combined likelihood ratio: 2 * 1.5 * 2 = 6
   - Posterior odds = Prior odds Ã— Combined likelihood ratio = 0.01 * 6 = 0.06
   - Posterior probability = 0.06 / (1 + 0.06) â‰ˆ 5.66%

5. **Red Team Thinking**
   - Overconfidence risk: The forecast might overestimate the impact of recent developments due to media amplification and expert biases.
   - Alternative scenario: Technological advances could mitigate risks, reducing the probability of extinction.
   - Adjustments: Consider the potential for mitigating factors and reduce the probability slightly to account for overestimation.

6. **Final Forecast**
   - After considering the base rate, evidence, and potential overconfidence, a calibrated probability of human extinction by 2100 is approximately 4%.

Final: 4%
```

### Claude-3.7-Sonnet (OR) [anthropic/claude-3.7-sonnet]
```
# Will humans go extinct before 2100?

## 1. Base Rate (Prior) Selection

For human extinction by 2100, I need to select an appropriate base rate. This is challenging since human extinction has never occurred, so I'll consider several reference points:

- Historical data: Humans have existed for roughly 300,000 years with no extinction events (0% historical rate)
- Metaculus community forecast: 1.0%
- Manifold market: 4.5%
- Oxford Global Catastrophic Risk Conference: 19% (likely selection-biased)
- Background extinction rate for mammalian species: ~0.1-1% per century

Given these considerations, I'll use a prior of 2%. This is:
- Higher than the historical rate (which is 0%)
- Close to the Metaculus prediction (1%)
- Lower than the Manifold market (4.5%)
- Much lower than the Oxford conference estimate (19%)
- Consistent with typical mammalian extinction rates but accounting for humans' unique adaptability

Prior probability: 2% (or odds of 1:49)

## 2. Comparison to Base Case

**Similarities to base case:**
- Humans remain highly adaptable with global presence across diverse environments
- We have survived previous existential threats (Cold War nuclear tensions, pandemics)
- Human population is large and geographically dispersed, providing resilience

**Differences from base case:**
- Unprecedented technological risks (especially AI) that didn't exist historically
- Climate change accelerating beyond historical precedents
- Increased nuclear proliferation and geopolitical tensions
- Greater global connectivity increasing pandemic spread potential
- Technological capabilities that could either mitigate or exacerbate risks

These differences suggest the need for adjustments to the base rate, particularly the novel risks from advanced AI and climate tipping points.

## 3. Evidence Evaluation (Likelihoods)

Let's evaluate key pieces of evidence:

**Evidence 1: AI experts warning of 20% chance of AI-induced extinction within 30 years**
- P(E|H): Very high (~0.9) - If extinction occurs, AI is a likely cause
- P(E|~H): Moderate (~0.1) - Experts often overestimate risks in their field
- Likelihood ratio: ~9 (strongly increases probability)

**Evidence 2: 70% chance of Gulf Stream collapse by 2035**
- P(E|H): Moderate (~0.6) - Gulf Stream collapse would be destabilizing but not directly extinction-causing
- P(E|~H): Moderate (~0.4) - Climate tipping points are concerning but humans could adapt
- Likelihood ratio: ~1.5 (slightly increases probability)

**Evidence 3: Nuclear war risks with massive casualties**
- P(E|H): High (~0.8) - Full-scale nuclear war is a plausible extinction pathway
- P(E|~H): Moderate (~0.3) - Nuclear deterrence has worked so far
- Likelihood ratio: ~2.7 (moderately increases probability)

**Evidence 4: Historical resilience of human species**
- P(E|H): Low (~0.2) - If extinction occurs, it would be despite our historical resilience
- P(E|~H): Very high (~0.9) - Our adaptability suggests continued survival
- Likelihood ratio: ~0.22 (decreases probability)

## 4. Bayesian Updating (Posterior Probability)

Starting with prior odds of 1:49 (2%), I'll update based on the evidence:

**Update on AI risk evidence:**
- Prior odds: 1:49
- Likelihood ratio: 9
- Posterior odds: 9:49 â‰ˆ 1:5.44
- Posterior probability: 1/(1+5.44) â‰ˆ 15.5%

**Update on Gulf Stream collapse:**
- Prior odds: 1:5.44
- Likelihood ratio: 1.5
- Posterior odds: 1.5:5.44 â‰ˆ 1:3.63
- Posterior probability: 1/(1+3.63) â‰ˆ 21.6%

**Update on nuclear war risks:**
- Prior odds: 1:3.63
- Likelihood ratio: 2.7
- Posterior odds: 2.7:3.63 â‰ˆ 1:1.34
- Posterior probability: 1/(1+1.34) â‰ˆ 42.7%

**Update on historical resilience:**
- Prior odds: 1:1.34
- Likelihood ratio: 0.22
- Posterior odds: 0.22:1.34 â‰ˆ 1:6.09
- Posterior probability: 1/(1+6.09) â‰ˆ 14.1%

## 5. Red Team Thinking

My analysis may have several weaknesses:

**Potential overconfidence:**
- The evidence on AI risk may be exaggerated by selection effects
- I may be overweighting recent alarming news versus long-term historical stability
- The likelihood ratios I assigned might be too extreme given deep uncertainty

**Blind spots:**
- I didn't fully account for unknown unknowns (e.g., unforeseen technological risks)
- I may be underestimating human adaptability and technological solutions
- The definition of "extinction" is binary, but partial collapses are more likely

**Alternative forecast:**
A reasonable alternative would be a much lower probability (1-5%) based on:
- Historical precedent of human survival
- Potential for technological solutions to emerging problems
- Geographic dispersion making complete extinction unlikely
- Metaculus community prediction of only 1%

**Rebuttals to alternative:**
- Historical precedent doesn't account for novel technological risks
- Even with geographic dispersion, global catastrophes could affect all populations
- Metaculus may underweight tail risks

Given these considerations, I should adjust my forecast downward from 14.1% to account for human adaptability and the extreme nature of complete extinction.

## 6. Final Forecast

After considering the base rate, evidence, and red team thinking, I believe the probability of human extinction by 2100 is approximately 8%. This reflects:

1. A low but non-negligible base rate
2. Significant evidence of novel existential risks (AI, climate, nuclear)
3. Counterbalancing evidence of human resilience and adaptability
4. Recognition that complete extinction is an extreme outcome requiring multiple failures

This forecast is higher than the Metaculus community (1%) but lower than the Manifold market (4.5%) and much lower than the Oxford conference (19%), reflecting my assessment that extinction risks are real but the complete extinction of humans is still unlikely given our adaptability and geographic dispersion.

Final: 8%
```

### Gemini [gemini-2.5-pro]
```
Here is my probabilistic reasoning process:

### 1. Base Rate (Prior) Selection
The event in question is human extinction within the next ~75 years. A purely historical base rate is zero, which is unhelpful as it doesn't account for novel technological risks. Expert opinion polls, like the 19% from the 2008 Oxford conference, are likely inflated due to strong selection bias.

A more robust starting point is the aggregated forecast from a large group of incentivized forecasters. The Metaculus community prediction of 1.0% serves as an excellent reference. However, to avoid being overly anchored and to allow for updates based on the provided evidence, I will select a slightly higher prior.

I will start with a prior probability of **2%**. This acknowledges that the risk is small but significant, and it is in the same ballpark as the most credible reference class (forecasting communities), while being slightly more pessimistic to account for the gravity of the risks to be discussed. In odds, this is 2/98, or approximately 1-to-49.

### 2. Comparison to Base Case
The base case can be considered the 20th century, where the primary existential risk was nuclear war. The current situation is different in several critical ways that increase the risk:
- **Artificial Intelligence**: The rapid development of potentially superhuman AI introduces a completely novel existential risk category that did not exist before.
- **Accelerated Climate Change**: The risk of hitting irreversible tipping points that could lead to civilizational collapse is now considered much higher and more imminent than in the past.
- **Geopolitical Instability**: The shift from a bipolar (Cold War) to a multipolar world may increase the chances of miscalculation and conflict between great powers.
- **Global Interconnectivity**: While it can aid cooperation, it also increases the speed at which pandemics or destabilizing information can spread.

These differences suggest that the probability of extinction is higher now than it was in the latter half of the 20th century.

### 3. Evidence Evaluation (Likelihoods)
I will evaluate the key pieces of new information from the research report.

*   **Evidence 1: AI experts warn of a 20% chance of AI-induced extinction within 30 years.**
    *   This is a very strong piece of evidence. If humans were to go extinct (H), it's highly likely that AI would be a leading cause, and we would expect to see experts in the field issuing dire warnings beforehand (P(E|H) is high). If we are not going to go extinct (~H), such specific and high-probability warnings from a group of experts are less likely, though not impossible (P(E|~H) is lower).
    *   The likelihood ratio is significantly greater than 1. This evidence strongly **increases** the probability.

*   **Evidence 2: Study shows a 70% chance of Gulf Stream collapse by 2035.**
    *   This points to a specific mechanism for catastrophic civilizational disruption. A collapse of this nature would lead to famine, mass migration, and war, which could then trigger other existential risks (like nuclear exchange). The link to *extinction* is indirect but plausible.
    *   P(E|H) is moderately high, as such a collapse is a plausible part of an extinction scenario. P(E|~H) is lower, as such a confident prediction of a major tipping point is less likely if the climate system were fundamentally stable.
    *   The likelihood ratio is greater than 1. This evidence **increases** the probability.

*   **Evidence 3: Market Consensus (Metaculus 1.0%, Manifold 4.5%).**
    *   This is a powerful piece of evidence representing the aggregated judgment of many individuals who have likely considered the same risks. The Metaculus forecast of 1.0% is a strong signal that the overall probability is low. The Manifold forecast of 4.5% suggests there is significant disagreement and that a higher probability is considered plausible.
    *   This evidence acts as a moderating force. It suggests that my initial prior of 2% is reasonable, but that a posterior in the double digits (which the AI/climate evidence might suggest in isolation) would be an overreaction. It pulls the final forecast towards the 1-5% range.

### 4. Bayesian Updating (Posterior Probability)
I start with my prior odds of 1-to-49 (2%).

The evidence from the AI expert poll and the Gulf Stream study provides a strong upward push. A 20% chance of AI extinction in 30 years is an alarming figure. If I were to update from this alone, the probability would rise dramatically. For instance, a likelihood ratio of 4 (i.e., this evidence is 4 times more likely if we go extinct than if we don't) would move the odds to 4-to-49, and the probability to ~7.5%. A further update for the climate risk would push it higher still, potentially over 10%.

However, this must be balanced against the market consensus. The Metaculus community is at 1%, and they are aware of these risks. This implies the community either disbelieves the high probabilities in these reports or believes in our ability to mitigate the risks. The Manifold market at 4.5% is a useful counterpoint, showing that a segment of the forecasting community is more alarmed.

My final posterior is a synthesis of these competing factors. I will update my 2% prior upwards significantly due to the severity of the AI and climate evidence, but I will remain anchored by the market data, preventing an update into the double digits. The Manifold forecast of 4.5% seems to strike a reasonable balance between the alarming new evidence and the more conservative long-term view. I will place my final estimate near that figure.

### 5. Red Team Thinking
My forecast of ~5% could be too high if I am overweighting recent, alarming headlines (availability bias) and underestimating human ingenuity and the high bar for *complete* extinction. The strongest argument against my forecast is the 1% on Metaculus, which represents the sober judgment of a large, incentivized crowd.

Conversely, my forecast could be too low. I might be failing to grasp the exponential pace of AI development and the possibility of correlated risks (e.g., climate crisis causing nuclear war). In this view, even 5% is a conservative estimate. The expert polls, while biased, come from those most familiar with the risks, and dismissing them entirely could be a mistake.

I believe that a 5% forecast appropriately balances these concerns. It treats extinction as a serious, non-trivial possibility (a 1-in-20 chance over the century) given the novel risks we face, which is a significant departure from historical norms, while not reaching the extreme levels of the most pessimistic expert polls.

### Final Forecast
My final probability reflects the initial base rate, updated by strong evidence of increased risk from AI and climate change, and then tempered by the wisdom of the crowds as reflected in prediction markets.

Final: 5%
```

### Grok [grok-4]
```
### 1. Base Rate (Prior) Selection
For the event of human extinction before 2100, an appropriate base rate should draw from historical precedents and expert/market consensus, as direct historical data on human extinction is zero (humanity has persisted through millennia despite various threats). The historical base rate for true extinction-level events is effectively 0% over recorded history (e.g., no global pandemics, nuclear wars, or asteroid impacts have wiped out all humans). However, expert estimates and prediction markets provide a more forward-looking reference class:

- The 2008 Oxford poll gave a median of 19%, but as noted, this is likely biased toward high estimates due to selection effects (attendees were already focused on catastrophic risks).
- More reliable are current prediction markets: Metaculus community forecast at 1% and Manifold at 4.5%, which aggregate diverse, incentivized opinions and tend to be well-calibrated for long-term forecasts.
- Broader expert syntheses (e.g., Toby Ord's "The Precipice" estimates ~16.7% by 2100, but this is on the higher end; other sources like the UN or IPCC imply much lower implicit risks for extinction).

I select a prior probability of 1% (odds 1:99), aligning closely with Metaculus as a conservative, market-informed base rate. This reflects the rarity of extinction events while acknowledging that unprecedented risks exist, but it avoids overindexing on biased high estimates.

### 2. Comparison to Base Case
The base case is humanity's historical survival through potential existential threats (e.g., the Black Death killed ~30-50% of Europe's population but not all humans; the Cold War involved nuclear brinkmanship but no extinction-level war). Similarities to the current situation include ongoing geopolitical tensions (e.g., like the Cold War) and natural disasters (e.g., pandemics like COVID-19, which was contained without extinction).

Differences, however, are significant and could increase the probability:
- Rapid AI development introduces novel, uncontrolled risks (unlike historical tech shifts, which were slower and more manageable).
- Accelerating climate change (e.g., potential Gulf Stream collapse) creates systemic, global instabilities not seen in past eras.
- Heightened nuclear risks in a multipolar world (e.g., more actors like Russia, China, North Korea) differ from the bipolar Cold War dynamic.
- Global connectivity amplifies pandemic spread, unlike isolated historical outbreaks.

These differences matter because they introduce "fat-tail" risksâ€”low-probability, high-impact events that could cascade into extinction, which weren't as prominent in the base case. This suggests a modest upward adjustment from the prior, but not a drastic one, as mitigating factors (e.g., international cooperation) have historically prevailed.

### 3. Evidence Evaluation (Likelihoods)
I'll evaluate key pieces of evidence from the research report, considering P(E | H) (likelihood if extinction occurs) vs. P(E | ~H) (if it doesn't), and the resulting likelihood ratio (LR = P(E | H) / P(E | ~H)). An LR >1 increases probability; <1 decreases it.

- **AI risks (large upward driver, e.g., 2025 expert warning of 20% AI-induced extinction in 30 years)**: P(E | H) is high (~80%) because uncontrolled AI could directly cause or contribute to extinction (e.g., via misaligned superintelligence). P(E | ~H) is moderate (~30%) as AI warnings occur even in non-extinction scenarios due to hype or caution. LR â‰ˆ 2.67 (increases probability moderately, as it's a novel risk but predictions vary widely).
- **Climate change impacts (moderate upward, e.g., 70% chance of Gulf Stream collapse by 2035)**: P(E | H) ~70% (severe shifts could lead to ecosystem collapse en route to extinction). P(E | ~H) ~40% (climate anomalies happen without full extinction, e.g., past ice ages). LR â‰ˆ 1.75 (increases probability slightly, but extinction-level climate events are speculative and mitigable).
- **Nuclear tensions (large upward, e.g., recent articles on war risks)**: P(E | H) ~90% (nuclear war could be a direct path to extinction via fallout/winter). P(E | ~H) ~50% (tensions rise and fall without war, as in the Cold War). LR â‰ˆ 1.8 (increases probability, but deterrence has historically worked).
- **Historical lack of extinction (small downward)**: P(E | H) ~10% (extinction would imply past near-misses were survived by chance). P(E | ~H) ~100% (no extinction is the norm). LR â‰ˆ 0.1 (decreases probability strongly, reinforcing that survival is the default).
- **Other factors (e.g., pandemics moderate upward, tech advances small mixed)**: Net LR ~1.2 (slight increase, as connectivity raises risks but vaccines/tech mitigate).

Overall, the evidence has a net positive LR (~2-3 combined), pulling toward higher probability due to emerging risks, but tempered by historical resilience.

### 4. Bayesian Updating (Posterior Probability)
Starting with prior P(H) = 1% (prior odds = 1:99).

For an explicit update, let's aggregate the upward evidence (AI, climate, nuclear) into a combined LR â‰ˆ 2.5 (conservative estimate: product of individual LRs, discounted for correlations). Include the historical downward evidence with LR â‰ˆ 0.5 (to avoid over-penalizing).

- Posterior odds = Prior odds Ã— LR = (1:99) Ã— 2.5 Ã— 0.5 = (1:99) Ã— 1.25 â‰ˆ 1.25:99 â‰ˆ 1:79.2
- Posterior P(H) = 1 / (1 + 79.2) â‰ˆ 1.25% (after this step).

A second qualitative update for remaining evidence (e.g., pandemics and connectivity) adds a small LR â‰ˆ 1.2: New posterior odds â‰ˆ 1:79.2 Ã— 1.2 â‰ˆ 1:66, so P(H) â‰ˆ 1.5%.

The resulting posterior is around 2%, reflecting a modest increase from the prior due to recent developments, but it remains highly uncertainâ€”extinction is still improbable, with wide error bars from data gaps and variable expert views.

### 5. Red Team Thinking
My forecast might be overconfident in the low prior (1%), potentially underweighting tail risks like cascading failures (e.g., AI + nuclear war combo), which could make true odds closer to 5-10% as in some expert views (e.g., Ord's). Blind spots include over-relying on markets, which can be optimistic or lag behind new evidence, and ignoring black-swan events like bioengineered pandemics. Alternative forecast: A higher 10% (e.g., if weighting the biased 19% poll more), plausible if recent AI/climate news signals a regime shift; rebuttal is that markets and history suggest overreaction, and mitigations (e.g., AI governance) are advancing. On the low end, 0.5% if emphasizing historical zero-base; rebuttal is that novel risks invalidate pure extrapolation. To account for tails, I widen my implicit confidence interval (e.g., 1-5% range) and nudge up slightly to 3% for robustness against underestimation.

### 6. Final Forecast
Final: 3%
```

